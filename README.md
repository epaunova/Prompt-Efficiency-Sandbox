# âš™ï¸ Prompt Efficiency Sandbox

This project compares different prompt engineering strategies by measuring:

- â±ï¸ Latency
- ğŸ“¦ Token usage
- ğŸ§  Output quality

It helps identify which prompts are more efficient and effective when interacting with large language models (LLMs) â€” balancing cost, speed, and clarity.

---

## ğŸ§  Prompt Variants Tested

1. **Simple Prompt**  
   _"Summarize the French Revolution."_

2. **Instructional Prompt**  
   _"Please provide a concise and well-structured summary of the key events and consequences of the French Revolution in under 100 words."_

3. **Chain-of-Thought Prompt**  
   _"Let's think step by step. First, outline the causes... Then describe key events... Finally, summarize outcomes."_

---

## ğŸ“Š Metrics Captured

- `Latency (s)` â€“ simulated response time
- `Tokens Used` â€“ simulated output length
- `Output Quality` â€“ scored on a 0.0â€“1.0 scale

All values are mocked for reproducibility, but the structure can be adapted for real LLM calls via OpenAI or other APIs.

---

## ğŸ“‚ Project Structure

prompt-efficiency-sandbox/
â”œâ”€â”€ scripts/ # Jupyter notebook and benchmark code
â”‚ â””â”€â”€ prompt_latency_comparison.ipynb
â”œâ”€â”€ results/ # Evaluation logs, token counts, latency samples
â”œâ”€â”€ models/ # Prompt templates, prompt-style configs
â”œâ”€â”€ evals/ # Manual or automated quality scoring
â””â”€â”€ README.md

yaml
Copy
Edit

---

## ğŸ““ Run the Notebook

```bash
cd scripts
jupyter notebook prompt_latency_comparison.ipynb
Youâ€™ll see:

Tabular comparison of 3 prompts

Bar charts for latency, token usage, and quality

ğŸ”— Related Project
ğŸ’¡ For full-scale evaluation with GPT-assisted grading, check out my LLM Evaluation Toolkit

ğŸ§ª Note
This is a sandbox project to demonstrate prompt efficiency benchmarking. Can be extended with:

Real LLM API integration

Token tracking (via tiktoken)

Output evaluation pipeline (manual or model-based)

ğŸ‘¤ Author
Eva Paunova
ğŸ”— LinkedIn
ğŸ“‚ Portfolio
