# âš™ï¸ Prompt Efficiency Sandbox

This project compares different prompt engineering strategies by measuring:

- â±ï¸ Latency
- ğŸ“¦ Token usage
- ğŸ§  Output quality

It demonstrates how prompt structure impacts performance, cost, and readability â€” essential for designing reliable LLM-based applications.

---

## ğŸ§  Prompt Variants Tested

1. **Simple Prompt**  
   `"Summarize the French Revolution."`

2. **Instructional Prompt**  
   `"Please provide a concise and well-structured summary of the key events..."`

3. **Chain-of-Thought Prompt**  
   `"Let's think step by step. First, outline the causes..."`

---

## ğŸ“Š Metrics Captured

| Metric         | Source                                 |
|----------------|----------------------------------------|
| Latency (s)    | Simulated, via `time.sleep()`          |
| Tokens Used    | Simulated integer range                |
| Output Quality | Manual scores (see `/evals`)           |

---

## ğŸ“ Project Structure

prompt-efficiency-sandbox/
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ prompt_latency_comparison.ipynb # Main notebook
â”œâ”€â”€ results/
â”‚ â””â”€â”€ prompt_eval_results.csv # Latency + token metrics
â”œâ”€â”€ models/
â”‚ â””â”€â”€ prompt_templates.json # Prompt definitions
â”œâ”€â”€ evals/
â”‚ â””â”€â”€ manual_output_scores.md # Manual evaluation
â””â”€â”€ README.md

yaml
Copy
Edit

---

## ğŸ““ How to Run

```bash
cd scripts
jupyter notebook prompt_latency_comparison.ipynb
Youâ€™ll see:

Table with prompt efficiency comparison

Charts for latency, token count, and output quality

ğŸ”— Key Files
ğŸ““ Example Notebook

ğŸ“„ Prompt Templates

ğŸ“Š Prompt Evaluation CSV

ğŸ§¾ Manual Quality Scores

ğŸ’¡ Related Projects
âœ¨ LLM Evaluation Toolkit â€” structured GPT-based output evaluation

ğŸ‘¤ Author
Eva Paunova
ğŸ”— LinkedIn
ğŸ“‚ Portfolio

